{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6cbe6131-f749-41f1-a098-a2051ee7708a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.cognitiveservices.vision.computervision import ComputerVisionClient\n",
    "from azure.cognitiveservices.vision.computervision.models import OperationStatusCodes\n",
    "from azure.cognitiveservices.vision.computervision.models import VisualFeatureTypes\n",
    "from msrest.authentication import CognitiveServicesCredentials\n",
    "\n",
    "from array import array\n",
    "import os\n",
    "from PIL import Image\n",
    "import sys\n",
    "import time\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "89a7b71e-44f1-4389-b6c1-08393c72d79b",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Authenticate\n",
    "Authenticates your credentials and creates a client.\n",
    "'''\n",
    "subscription_key = \"d5826fda08494f0aa4d710c21fa1e415\"\n",
    "endpoint = \"https://sami-compviz.cognitiveservices.azure.com/\"\n",
    "\n",
    "computervision_client = ComputerVisionClient(endpoint, CognitiveServicesCredentials(subscription_key))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "12b7c3b2-230f-4c37-aadd-0522008d1490",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== Read File - remote =====\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "OCR: Read File using the Read API, extract text - remote\n",
    "This example will extract text in an image, then print results, line by line.\n",
    "This API call can also extract handwriting style text (not shown).\n",
    "'''\n",
    "print(\"===== Read File - remote =====\")\n",
    "# Get an image with text\n",
    "read_image_url = \"https://raw.githubusercontent.com/MicrosoftDocs/azure-docs/master/articles/cognitive-services/Computer-vision/Images/readsample.jpg\"\n",
    "\n",
    "# Call API with URL and raw response (allows you to get the operation location)\n",
    "read_response = computervision_client.read(read_image_url,  raw=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d75b228b-a9f9-417e-8b80-ba40a0e554e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The quick brown fox jumps\n",
      "[38.0, 650.0, 2572.0, 699.0, 2570.0, 854.0, 37.0, 815.0]\n",
      "Over\n",
      "[184.0, 1053.0, 508.0, 1044.0, 510.0, 1123.0, 184.0, 1128.0]\n",
      "the lazy dog!\n",
      "[639.0, 1011.0, 1976.0, 1026.0, 1974.0, 1158.0, 637.0, 1141.0]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Get the operation location (URL with an ID at the end) from the response\n",
    "read_operation_location = read_response.headers[\"Operation-Location\"]\n",
    "# Grab the ID from the URL\n",
    "operation_id = read_operation_location.split(\"/\")[-1]\n",
    "\n",
    "# Call the \"GET\" API and wait for it to retrieve the results \n",
    "while True:\n",
    "    read_result = computervision_client.get_read_result(operation_id)\n",
    "    if read_result.status not in ['notStarted', 'running']:\n",
    "        break\n",
    "    time.sleep(1)\n",
    "\n",
    "# Print the detected text, line by line\n",
    "if read_result.status == OperationStatusCodes.succeeded:\n",
    "    for text_result in read_result.analyze_result.read_results:\n",
    "        for line in text_result.lines:\n",
    "            print(line.text)\n",
    "            print(line.bounding_box)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cf9e0d8-8921-4f08-87e8-98ff4dc821f2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "id": "f003cab3-68b1-48eb-a881-d45f51502520",
   "metadata": {},
   "source": [
    "Request URL\n",
    "https://{endpoint}/vision/v3.2/read/analyze[?language][&pages][&readingOrder][&model-version]\n",
    "Request parameters\n",
    "language (optional)string\n",
    "See https://aka.ms/ocr-languages for list of supported languages.\n",
    "\n",
    "pages (optional)string\n",
    "The page selection only leveraged for multi-page PDF and TIFF documents. Accepted input include single pages (e.g.'1, 2' -> pages 1 and 2 will be processed), finite (e.g. '2-5' -> pages 2 to 5 will be processed) and open-ended ranges (e.g. '5-' -> all the pages from page 5 will be processed & e.g. '-10' -> pages 1 to 10 will be processed). All of these can be mixed together and ranges are allowed to overlap (eg. '-5, 1, 3, 5-10' - pages 1 to 10 will be processed). The service will accept the request if it can process at least one page of the document (e.g. using '5-100' on a 5 page document is a valid input where page 5 will be processed). If no page range is provided, the entire document will be processed.\n",
    "\n",
    "readingOrder (optional)string\n",
    "Optional parameter to specify which reading order algorithm should be applied when ordering the extract text elements. Can be either 'basic' or 'natural'. Will default to basic if not specified\n",
    "\n",
    "model-version (optional)string\n",
    "Optional parameter to specify the version of the OCR model used to extract text information for the image/document submitted. Accepted values are: \"latest\", \"2021-04-12\", \"2021-09-30-preview\", \"2022-01-30-preview\". Defaults to latest if not provided.\n",
    "\n",
    "Request headers\n",
    "Content-TypestringMedia type of the body sent to the API.\n",
    "Ocp-Apim-Subscription-KeystringSubscription key which provides access to this API. Found in your Cognitive Services accounts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "be4ca57b-ff90-4f43-ba7a-d0c6e283cb1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b''\n"
     ]
    }
   ],
   "source": [
    "####################################\n",
    "# READ API\n",
    "########### Python 3.2 #############\n",
    "\n",
    "import http.client, urllib.request, urllib.parse, urllib.error, base64\n",
    "\n",
    "headers = {\n",
    "    # Request headers\n",
    "    'Content-Type': 'application/json',\n",
    "    'Ocp-Apim-Subscription-Key': 'd5826fda08494f0aa4d710c21fa1e415',\n",
    "}\n",
    "\n",
    "params = urllib.parse.urlencode({\n",
    "    # Request parameters\n",
    "    'language': 'en',\n",
    "    'pages': '1',\n",
    "    'readingOrder': 'natural',\n",
    "    'model-version': 'latest',\n",
    "})\n",
    "\n",
    "try:\n",
    "    conn = http.client.HTTPSConnection('eastus.api.cognitive.microsoft.com')\n",
    "   # print(\"POST\", \"/vision/v3.2/read/analyze?%s\" % params, '{\"url\":\"https://intelligentkioskstore.blob.core.windows.net/visionapi/suggestedphotos/3.png\"}', headers)\n",
    "    conn.request(\"POST\", \"/vision/v3.2/read/analyze?%s\" % params, '{\"url\":\"https://raw.githubusercontent.com/MicrosoftDocs/azure-docs/master/articles/cognitive-services/Computer-vision/Images/readsample.jpg\"}', headers)\n",
    "    response = conn.getresponse()\n",
    "    data = response.read()\n",
    "    print(data)\n",
    "    conn.close()\n",
    "except Exception as e:\n",
    "    print(\"[Errno {0}] {1}\".format(e.errno, e.strerror))\n",
    "\n",
    "####################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62623eba-6a71-49f3-819b-1145e61002ac",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
